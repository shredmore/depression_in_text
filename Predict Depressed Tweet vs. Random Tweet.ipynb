{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depression in Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/benthompson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/benthompson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benthompson/anaconda/envs/pi/lib/python3.4/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/benthompson/anaconda/envs/pi/lib/python3.4/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# import nltk library\n",
    "import nltk; nltk.download('punkt')\n",
    "from nltk import sent_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "\n",
    "# import stopword libraries\n",
    "nltk.download('stopwords'); from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "# import other libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# import word embedding library\n",
    "#import glove_helper\n",
    "\n",
    "# import helper libraries\n",
    "import collections\n",
    "from common import utils, vocabulary\n",
    "\n",
    "#display multiple results per cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "#export models\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in tweets\n",
    "df = pd.DataFrame.from_csv('/Users/benthompson/depression_tweets.csv', header=None, parse_dates=True, infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add index\n",
    "df = df.reset_index()\n",
    "\n",
    "#set column names\n",
    "df.columns = ['date','tweet_id', 'handle', 'id', 'tweet', 'language', 'device', 'notes', 'notes_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>handle</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>language</th>\n",
       "      <th>device</th>\n",
       "      <th>notes</th>\n",
       "      <th>notes_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-04-05 19:14:48</td>\n",
       "      <td>981973445616525312</td>\n",
       "      <td>Haldol</td>\n",
       "      <td>816793117785542656</td>\n",
       "      <td>Currently I am on 150 mg of hydroxyzine for in...</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-04-05 19:14:48</td>\n",
       "      <td>981973444723064832</td>\n",
       "      <td>Rick O</td>\n",
       "      <td>3192532759</td>\n",
       "      <td>Integrated behavioral health for POLICE. Treat...</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-04-05 19:14:47</td>\n",
       "      <td>981973443988996096</td>\n",
       "      <td>olivia üßùüèΩ‚Äç‚ôÄÔ∏è„Éú„Çπ</td>\n",
       "      <td>1321438920</td>\n",
       "      <td>RT @DevinnJay: I won‚Äôt allow depression to fuc...</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-04-05 19:14:47</td>\n",
       "      <td>981973443154505728</td>\n",
       "      <td>LeFrenchNeuropsy</td>\n",
       "      <td>2887994266</td>\n",
       "      <td>RT @LePsylab: For science ! Un questionnaire p...</td>\n",
       "      <td>fr</td>\n",
       "      <td>Twitter Web Client</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-04-05 19:14:45</td>\n",
       "      <td>981973435705421826</td>\n",
       "      <td>GEEZ</td>\n",
       "      <td>311289251</td>\n",
       "      <td>I lost my brova I fell deep in depression!</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date            tweet_id            handle  \\\n",
       "0 2018-04-05 19:14:48  981973445616525312            Haldol   \n",
       "1 2018-04-05 19:14:48  981973444723064832            Rick O   \n",
       "2 2018-04-05 19:14:47  981973443988996096    olivia üßùüèΩ‚Äç‚ôÄÔ∏è„Éú„Çπ   \n",
       "3 2018-04-05 19:14:47  981973443154505728  LeFrenchNeuropsy   \n",
       "4 2018-04-05 19:14:45  981973435705421826              GEEZ   \n",
       "\n",
       "                   id                                              tweet  \\\n",
       "0  816793117785542656  Currently I am on 150 mg of hydroxyzine for in...   \n",
       "1          3192532759  Integrated behavioral health for POLICE. Treat...   \n",
       "2          1321438920  RT @DevinnJay: I won‚Äôt allow depression to fuc...   \n",
       "3          2887994266  RT @LePsylab: For science ! Un questionnaire p...   \n",
       "4           311289251         I lost my brova I fell deep in depression!   \n",
       "\n",
       "  language               device  notes  notes_2  \n",
       "0       en   Twitter for iPhone    NaN      NaN  \n",
       "1       en   Twitter for iPhone    NaN      NaN  \n",
       "2       en   Twitter for iPhone    NaN      NaN  \n",
       "3       fr   Twitter Web Client    NaN      NaN  \n",
       "4       en  Twitter for Android    NaN      NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at data\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2839435"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how man non-distinct tweets\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter to english only\n",
    "df = df[df['language'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2576632"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many tweets now\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ".                    5508\n",
       "Aiden Hatfield       3004\n",
       "Ÿã                    2653\n",
       "In Music We Trust    2258\n",
       "‚ô°                    1700\n",
       "Name: handle, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#any users w/lots of tweets that might skew model?\n",
    "#not any that seem too high\n",
    "df['handle'].value_counts().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1037577"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many distinct tweets\n",
    "len(df.tweet.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make distinct tweets the df\n",
    "df = pd.DataFrame(df.tweet.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rename columns\n",
    "df.columns = ['tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#export sample to check quality\n",
    "# pd.options.display.max_colwidth = 1000\n",
    "# df_sample = df.sample(n=100)\n",
    "# df_sample.to_csv('../sample_100_depression_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweets    Y‚Äôall be using mental illness as a way to justify your bitchy behavior and it‚Äôs honestly a no from fucking me. Depr‚Ä¶ https://t.co/YzlhNSV9RA\n",
       "Name: 45055, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look up specific tweet\n",
    "pd.options.display.max_colwidth = 10000\n",
    "df.iloc[45055]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create column on 1's\n",
    "x = [1]\n",
    "x = x * len(df)\n",
    "df['target'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Currently I am on 150 mg of hydroxyzine for insomnia.  As well as 300 mg Effexor XR and 4 mg Fanapt for Psychotic Depression.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Integrated behavioral health for POLICE. Treat mind &amp;amp; body. The organizational &amp;amp; operational stressors have long te‚Ä¶ https://t.co/MnWDFqckQB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @DevinnJay: I won‚Äôt allow depression to fuck me up &amp;amp; set me back. Nah  not again.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I lost my brova I fell deep in depression!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @peachesfrfr: so there i am  depression all over my titties</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                 tweets  \\\n",
       "0                         Currently I am on 150 mg of hydroxyzine for insomnia.  As well as 300 mg Effexor XR and 4 mg Fanapt for Psychotic Depression.   \n",
       "1  Integrated behavioral health for POLICE. Treat mind &amp; body. The organizational &amp; operational stressors have long te‚Ä¶ https://t.co/MnWDFqckQB   \n",
       "2                                                              RT @DevinnJay: I won‚Äôt allow depression to fuck me up &amp; set me back. Nah  not again.   \n",
       "3                                                                                                            I lost my brova I fell deep in depression!   \n",
       "4                                                                                        RT @peachesfrfr: so there i am  depression all over my titties   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring in random tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in tweets\n",
    "df_2 = pd.DataFrame.from_csv('../random_tweets.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True but she still cancelled tho.</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT @roxxxdoxxx: when she said \"i gotta ask first\" i felt that üò´üòÖüòÇ https://t.co/BGPZqFLb9v</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>appreciate this perfectly timed pic of me and catto pls https://t.co/GE5poooRcF</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one of Beyonc√©‚Äôs most underrated looks is the one from Jealous. don‚Äôt @ me</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"Once you create a system for censoring speech on the grounds that it is 'fake news' (even if it's parody, or sarca‚Ä¶ https://t.co/EpuSUaK0UC</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [True but she still cancelled tho., RT @roxxxdoxxx: when she said \"i gotta ask first\" i felt that üò´üòÖüòÇ https://t.co/BGPZqFLb9v, appreciate this perfectly timed pic of me and catto pls https://t.co/GE5poooRcF, one of Beyonc√©‚Äôs most underrated looks is the one from Jealous. don‚Äôt @ me, \"Once you create a system for censoring speech on the grounds that it is 'fake news' (even if it's parody, or sarca‚Ä¶ https://t.co/EpuSUaK0UC]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at data\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135177"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many\n",
    "len(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#give index\n",
    "df_2 = df_2.reset_index()\n",
    "\n",
    "#give column name\n",
    "df_2.columns = ['tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111985"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many distinct tweets\n",
    "len(df_2.tweets.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make dataframe of unique\n",
    "df_2 = pd.DataFrame(df_2.tweets.unique())\n",
    "\n",
    "#give column name\n",
    "df_2.columns = ['tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make all tweets lowercase\n",
    "df_2['tweets'] = df_2['tweets'].str.lower()\n",
    "df_2.columns = ['tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>true but she still cancelled tho.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rt @roxxxdoxxx: when she said \"i gotta ask first\" i felt that üò´üòÖüòÇ https://t.co/bgpzqflb9v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>appreciate this perfectly timed pic of me and catto pls https://t.co/ge5pooorcf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>one of beyonc√©‚Äôs most underrated looks is the one from jealous. don‚Äôt @ me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"once you create a system for censoring speech on the grounds that it is 'fake news' (even if it's parody, or sarca‚Ä¶ https://t.co/epusuak0uc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                         tweets\n",
       "0                                                                                                             true but she still cancelled tho.\n",
       "1                                                     rt @roxxxdoxxx: when she said \"i gotta ask first\" i felt that üò´üòÖüòÇ https://t.co/bgpzqflb9v\n",
       "2                                                               appreciate this perfectly timed pic of me and catto pls https://t.co/ge5pooorcf\n",
       "3                                                                    one of beyonc√©‚Äôs most underrated looks is the one from jealous. don‚Äôt @ me\n",
       "4  \"once you create a system for censoring speech on the grounds that it is 'fake news' (even if it's parody, or sarca‚Ä¶ https://t.co/epusuak0uc"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>rt @nickhansonmn: hey sorry i‚Äôve been distant lately , i‚Äôm just super depressed about the current state of my life and didn‚Äôt wanna have to‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>rt @matrix_reioaded: ahh i‚Äôm depressed... but all my teachers in elementary school said i was a special boy... how could this happen.. at o‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2568</th>\n",
       "      <td>rt @caucasianjames: on tinder depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2763</th>\n",
       "      <td>rt @softyoonle: hi this would really help me alot since i am battling depression and i need something to inspire me or help me üíû if this fl‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3507</th>\n",
       "      <td>rt @iamsofiadg: philippines üáµüá≠ \\nneed someone to talk to?\\nsuicide/depression cellphone  number \\n\\n0917-558-4673 üì±\\n\\ncan you rt to potentially‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4159</th>\n",
       "      <td>rt @depressionnote: warning signs of depression ‚ö†Ô∏è\\n\\n‚ö†Ô∏è low self-esteem\\n‚ö†Ô∏è guilt\\n‚ö†Ô∏è feeling hopeless\\n‚ö†Ô∏è tiredness\\n‚ö†Ô∏è loss of interest in thi‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4268</th>\n",
       "      <td>rt @fuxksalliemae: alot of nigerians are struggling with depression, and much of it is financially and economically induced. don't just che‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4547</th>\n",
       "      <td>rt @daitonreed: you do not have to come this hard on every song. aint no damn reason you needa make me depressed for no reason. https://t.c‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5501</th>\n",
       "      <td>rt @sionesnow: when i read the first sentence i thought there was a thing called ‚Äúdepression fcking‚Äù weh lol https://t.co/yup73ftjqc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6143</th>\n",
       "      <td>i can't help the fact that i make all my characters depressed or not human, because i just realized that alexandria‚Ä¶ https://t.co/scwoudncxy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6601</th>\n",
       "      <td>rt @prince_madness1: know the signs of depression:\\n\\nfatigue. feelings of guilt, worthlessness, and hopelessness. insomnia, or sleeping too‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6907</th>\n",
       "      <td>rt @depresseddarth: the dark side of the moon https://t.co/wy7qfybrx1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8795</th>\n",
       "      <td>rt @kanyepodcast: @kanyewest unfortunately, it‚Äôs not always that easy for everyone. some depression is genetic, and that shit takes medicin‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8978</th>\n",
       "      <td>@lin_manuel man i wish. sometimes i feel like i'm repeating what every depressed teen says. but thanks for the positivity anyways.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8983</th>\n",
       "      <td>rt @imtheebrock: me 5 minutes after being depressed for no reason https://t.co/1dzk822v8r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10191</th>\n",
       "      <td>kids who still exist from elementary have depression. i couldn't read for shit in elementary; there's no correlatio‚Ä¶ https://t.co/c8z376lmbn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10595</th>\n",
       "      <td>i‚Äôm fucking depressed i had to pass up on beyonc√© to fuckinggg adulttt what‚Äôs life üò≠ü§¨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11851</th>\n",
       "      <td>let‚Äôs talk about how i was so depressed that i started to feel disconnected from my body and convinced myself that‚Ä¶ https://t.co/hakcnpehlf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13416</th>\n",
       "      <td>‚Ä¢ fights oxidative stress in the body\\n‚Ä¢ may control inflammation\\n‚Ä¢ anti-aging\\n‚Ä¢ may prevent depression and stress\\n‚Ä¢‚Ä¶ https://t.co/5naylxcwup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13514</th>\n",
       "      <td>rt @bimtrimmer: rt if u\\n\\n- broke\\n- want concert tickets\\n- want to meet your fav\\n- depressed\\n- gay\\n- ugly as hell\\n- want pizza rolls\\n\\nno one‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13878</th>\n",
       "      <td>rt @depresseddarth: when it's friday night but you have no friends https://t.co/gunegzdz9h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14291</th>\n",
       "      <td>rt @_kelliraee_: this is what defeating depression looks like... https://t.co/kknfcsbiy1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14563</th>\n",
       "      <td>rt @sopetacles: hello! my name is jyn and i‚Äôm currently losing my long battle with severe depression. this may look like a small help but i‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>rt @depresseddarth: when it's friday night but you have no plans cause all your friends died in the death star https://t.co/flhy2eemwj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15611</th>\n",
       "      <td>rt @wearegraves: i can‚Äôt wait to start releasing music from my next project i‚Äôve been writing with people who‚Äôs depressed and fuck up as me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17008</th>\n",
       "      <td>hey... so thank you @bryandechart for just helping me not... self harm. rn im really depressed... but seeing you li‚Ä¶ https://t.co/4dyj9cashj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18377</th>\n",
       "      <td>rt @depresseddarth: ‚Äúhello elon, can you please invent lightsabers, thanks‚Äù https://t.co/scuslea06v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18978</th>\n",
       "      <td>rt @baba_fatymah: @ayshaummy4 @zikirillahi allah sarki \\nmy little brother is 9 i can't imagine him being depressed \\nallah ya shirya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20281</th>\n",
       "      <td>rt @youthkiawaaz: let's talk about depression.\\nlet's talk about caste and privilege. \\nlet's talk about trans representation. \\nlet's talk ab‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23597</th>\n",
       "      <td>been unhappy and depressed for so long i don't know what happiness feels like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91929</th>\n",
       "      <td>i‚Äôm high key depressed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92002</th>\n",
       "      <td>rt @depressionnote: today i feel\\n\\n‚Ä¢ abandoned\\n‚Ä¢ ugly\\n‚Ä¢ hurt\\n‚Ä¢ like i don‚Äôt matter\\n‚Ä¢ invisible \\n‚Ä¢ useless\\n‚Ä¢ like i don‚Äôt belong\\n‚Ä¢ not worthy‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92107</th>\n",
       "      <td>rt @depressionnote: it doesn‚Äôt matter what race, gender, sexuality you are or how much money you have. anyone can experience mental health‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92576</th>\n",
       "      <td>rt @camhnews: 'kate spade‚Äôs death proves depression doesn‚Äôt just affect the poor, unsuccessful' via @globeandmail. camh's @drkatykamkar say‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92584</th>\n",
       "      <td>rt @stevenspohn: the most successful people i've met:\\n\\n1. fight depression\\n2. stress over maintaining quality\\n3. care about others\\n4. doubt‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92904</th>\n",
       "      <td>rt @itsrjhill: i‚Äôm so depressed yo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93164</th>\n",
       "      <td>rt @nikkilipstick: ‚ú®üñ§üéÄwhen you finally get a handle on your depression üéÄüñ§‚ú® https://t.co/vpacsorqbb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93698</th>\n",
       "      <td>@freshempire how come there's not a depressed answer?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94878</th>\n",
       "      <td>i feel like i just hit straight random depression...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94982</th>\n",
       "      <td>rt @v_nooguyen: one of my professors wrote a thing on what it‚Äôs like to have depression. for people who keep saying shit like ‚Äúi just don‚Äôt‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96304</th>\n",
       "      <td>rt @supremeshana_: religion doesn't cure depression y'all, seek real help. \\ndon't @ me. https://t.co/uh5rqn1lh7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96340</th>\n",
       "      <td>rt @drgnkiller: i like how he starts with ‚Äúi‚Äôm not using this as an excuse buuut...‚Äù then goes on to say depression caused him to be an ass‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97603</th>\n",
       "      <td>rt @a_joseph1616: \"thanks to psychedelic drugs, 'treatment-resistant' depression is no longer a thing\"\\n\\nhttps://t.co/2q7a0vmksr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97894</th>\n",
       "      <td>the intensity of my depression tends to strike‚Ä¶ https://t.co/bv63zdm3ec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98085</th>\n",
       "      <td>rt @sidayaofficial: causes of insomnia: \\n\\nregret. \\nself-blame. \\noverthinking. \\nanger. \\ndepression. \\nloneliness. \\nand\\nyou.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100539</th>\n",
       "      <td>rt @sierracheyanne_: i hope this shit makes me cry in a none depressed kinda way. https://t.co/ct0wm8ud02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101869</th>\n",
       "      <td>rt @athariyyah_: it's actually bare upsetting when you see muslims suffering from mental illnesses like depression etc. what's even more sa‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103044</th>\n",
       "      <td>rt @tonysraptorsofc: i wish there was one day i don‚Äôt feel sad or depressed. that i can be happy just for one whole day. #tuloyparinkisston‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103322</th>\n",
       "      <td>rt @leelamad: @nazeeahmed no run means i am depressed! so whatever it is i gotta be outside running! üòä</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105468</th>\n",
       "      <td>off work finally yes now time to go home and lay fully clothed over my covers staring at the wall while i wallow in my depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105472</th>\n",
       "      <td>rt @asapyams: very depressed writing in lower case until i find happiness again bruh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106295</th>\n",
       "      <td>@19chelsea94 i quit my job recently to focus on bettering myself. trying to get ptsd and anxiety/depression under control.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106921</th>\n",
       "      <td>that‚Äôs called depression lol https://t.co/fhmut7tupc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107568</th>\n",
       "      <td>rt @spacebrat: its funny how u get depressed and just start not talking to people but u missing talking and interacting with then but u jus‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107822</th>\n",
       "      <td>rt @itzwikipedia: too much homework can cause stress, depression, and even lower grades.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108714</th>\n",
       "      <td>i‚Äôm so tired of being a worthless piece of shit but my depression is literally destroying me and i haven‚Äôt done any‚Ä¶ https://t.co/x9e93ezqmu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109421</th>\n",
       "      <td>trump's america wants to get rid of coverage for these drugs because apparently depression is a pre-existing condit‚Ä¶ https://t.co/zzapyrmlvm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110060</th>\n",
       "      <td>rt @rosyfuck: @bbzaria @aljaminbrydie omg did u just end my depression üòãüíóüòçüòäüòçüò§‚òïÔ∏è‚ù§Ô∏è‚ù£Ô∏èü§™</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110185</th>\n",
       "      <td>this is for those, who are sad, frustrated, alone, depressed because of many reasons like for being unemployed, uns‚Ä¶ https://t.co/scbsnaxi9y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111439</th>\n",
       "      <td>shoutout to being depressed and malnourished for this weight loss</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>138 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                        tweets\n",
       "1769              rt @nickhansonmn: hey sorry i‚Äôve been distant lately , i‚Äôm just super depressed about the current state of my life and didn‚Äôt wanna have to‚Ä¶\n",
       "1918              rt @matrix_reioaded: ahh i‚Äôm depressed... but all my teachers in elementary school said i was a special boy... how could this happen.. at o‚Ä¶\n",
       "2568                                                                                                                   rt @caucasianjames: on tinder depressed\n",
       "2763              rt @softyoonle: hi this would really help me alot since i am battling depression and i need something to inspire me or help me üíû if this fl‚Ä¶\n",
       "3507         rt @iamsofiadg: philippines üáµüá≠ \\nneed someone to talk to?\\nsuicide/depression cellphone  number \\n\\n0917-558-4673 üì±\\n\\ncan you rt to potentially‚Ä¶\n",
       "4159        rt @depressionnote: warning signs of depression ‚ö†Ô∏è\\n\\n‚ö†Ô∏è low self-esteem\\n‚ö†Ô∏è guilt\\n‚ö†Ô∏è feeling hopeless\\n‚ö†Ô∏è tiredness\\n‚ö†Ô∏è loss of interest in thi‚Ä¶\n",
       "4268              rt @fuxksalliemae: alot of nigerians are struggling with depression, and much of it is financially and economically induced. don't just che‚Ä¶\n",
       "4547              rt @daitonreed: you do not have to come this hard on every song. aint no damn reason you needa make me depressed for no reason. https://t.c‚Ä¶\n",
       "5501                      rt @sionesnow: when i read the first sentence i thought there was a thing called ‚Äúdepression fcking‚Äù weh lol https://t.co/yup73ftjqc\n",
       "6143              i can't help the fact that i make all my characters depressed or not human, because i just realized that alexandria‚Ä¶ https://t.co/scwoudncxy\n",
       "6601             rt @prince_madness1: know the signs of depression:\\n\\nfatigue. feelings of guilt, worthlessness, and hopelessness. insomnia, or sleeping too‚Ä¶\n",
       "6907                                                                                     rt @depresseddarth: the dark side of the moon https://t.co/wy7qfybrx1\n",
       "8795              rt @kanyepodcast: @kanyewest unfortunately, it‚Äôs not always that easy for everyone. some depression is genetic, and that shit takes medicin‚Ä¶\n",
       "8978                        @lin_manuel man i wish. sometimes i feel like i'm repeating what every depressed teen says. but thanks for the positivity anyways.\n",
       "8983                                                                 rt @imtheebrock: me 5 minutes after being depressed for no reason https://t.co/1dzk822v8r\n",
       "10191             kids who still exist from elementary have depression. i couldn't read for shit in elementary; there's no correlatio‚Ä¶ https://t.co/c8z376lmbn\n",
       "10595                                                                    i‚Äôm fucking depressed i had to pass up on beyonc√© to fuckinggg adulttt what‚Äôs life üò≠ü§¨\n",
       "11851              let‚Äôs talk about how i was so depressed that i started to feel disconnected from my body and convinced myself that‚Ä¶ https://t.co/hakcnpehlf\n",
       "13416         ‚Ä¢ fights oxidative stress in the body\\n‚Ä¢ may control inflammation\\n‚Ä¢ anti-aging\\n‚Ä¢ may prevent depression and stress\\n‚Ä¢‚Ä¶ https://t.co/5naylxcwup\n",
       "13514   rt @bimtrimmer: rt if u\\n\\n- broke\\n- want concert tickets\\n- want to meet your fav\\n- depressed\\n- gay\\n- ugly as hell\\n- want pizza rolls\\n\\nno one‚Ä¶\n",
       "13878                                                               rt @depresseddarth: when it's friday night but you have no friends https://t.co/gunegzdz9h\n",
       "14291                                                                 rt @_kelliraee_: this is what defeating depression looks like... https://t.co/kknfcsbiy1\n",
       "14563             rt @sopetacles: hello! my name is jyn and i‚Äôm currently losing my long battle with severe depression. this may look like a small help but i‚Ä¶\n",
       "14996                   rt @depresseddarth: when it's friday night but you have no plans cause all your friends died in the death star https://t.co/flhy2eemwj\n",
       "15611             rt @wearegraves: i can‚Äôt wait to start releasing music from my next project i‚Äôve been writing with people who‚Äôs depressed and fuck up as me.\n",
       "17008             hey... so thank you @bryandechart for just helping me not... self harm. rn im really depressed... but seeing you li‚Ä¶ https://t.co/4dyj9cashj\n",
       "18377                                                      rt @depresseddarth: ‚Äúhello elon, can you please invent lightsabers, thanks‚Äù https://t.co/scuslea06v\n",
       "18978                    rt @baba_fatymah: @ayshaummy4 @zikirillahi allah sarki \\nmy little brother is 9 i can't imagine him being depressed \\nallah ya shirya\n",
       "20281          rt @youthkiawaaz: let's talk about depression.\\nlet's talk about caste and privilege. \\nlet's talk about trans representation. \\nlet's talk ab‚Ä¶\n",
       "23597                                                                            been unhappy and depressed for so long i don't know what happiness feels like\n",
       "...                                                                                                                                                        ...\n",
       "91929                                                                                                                                  i‚Äôm high key depressed.\n",
       "92002    rt @depressionnote: today i feel\\n\\n‚Ä¢ abandoned\\n‚Ä¢ ugly\\n‚Ä¢ hurt\\n‚Ä¢ like i don‚Äôt matter\\n‚Ä¢ invisible \\n‚Ä¢ useless\\n‚Ä¢ like i don‚Äôt belong\\n‚Ä¢ not worthy‚Ä¶\n",
       "92107              rt @depressionnote: it doesn‚Äôt matter what race, gender, sexuality you are or how much money you have. anyone can experience mental health‚Ä¶\n",
       "92576             rt @camhnews: 'kate spade‚Äôs death proves depression doesn‚Äôt just affect the poor, unsuccessful' via @globeandmail. camh's @drkatykamkar say‚Ä¶\n",
       "92584        rt @stevenspohn: the most successful people i've met:\\n\\n1. fight depression\\n2. stress over maintaining quality\\n3. care about others\\n4. doubt‚Ä¶\n",
       "92904                                                                                                                       rt @itsrjhill: i‚Äôm so depressed yo\n",
       "93164                                                       rt @nikkilipstick: ‚ú®üñ§üéÄwhen you finally get a handle on your depression üéÄüñ§‚ú® https://t.co/vpacsorqbb\n",
       "93698                                                                                                    @freshempire how come there's not a depressed answer?\n",
       "94878                                                                                                     i feel like i just hit straight random depression...\n",
       "94982             rt @v_nooguyen: one of my professors wrote a thing on what it‚Äôs like to have depression. for people who keep saying shit like ‚Äúi just don‚Äôt‚Ä¶\n",
       "96304                                         rt @supremeshana_: religion doesn't cure depression y'all, seek real help. \\ndon't @ me. https://t.co/uh5rqn1lh7\n",
       "96340             rt @drgnkiller: i like how he starts with ‚Äúi‚Äôm not using this as an excuse buuut...‚Äù then goes on to say depression caused him to be an ass‚Ä¶\n",
       "97603                        rt @a_joseph1616: \"thanks to psychedelic drugs, 'treatment-resistant' depression is no longer a thing\"\\n\\nhttps://t.co/2q7a0vmksr\n",
       "97894                                                                                  the intensity of my depression tends to strike‚Ä¶ https://t.co/bv63zdm3ec\n",
       "98085                       rt @sidayaofficial: causes of insomnia: \\n\\nregret. \\nself-blame. \\noverthinking. \\nanger. \\ndepression. \\nloneliness. \\nand\\nyou.\n",
       "100539                                               rt @sierracheyanne_: i hope this shit makes me cry in a none depressed kinda way. https://t.co/ct0wm8ud02\n",
       "101869            rt @athariyyah_: it's actually bare upsetting when you see muslims suffering from mental illnesses like depression etc. what's even more sa‚Ä¶\n",
       "103044            rt @tonysraptorsofc: i wish there was one day i don‚Äôt feel sad or depressed. that i can be happy just for one whole day. #tuloyparinkisston‚Ä¶\n",
       "103322                                                  rt @leelamad: @nazeeahmed no run means i am depressed! so whatever it is i gotta be outside running! üòä\n",
       "105468                       off work finally yes now time to go home and lay fully clothed over my covers staring at the wall while i wallow in my depression\n",
       "105472                                                                    rt @asapyams: very depressed writing in lower case until i find happiness again bruh\n",
       "106295                              @19chelsea94 i quit my job recently to focus on bettering myself. trying to get ptsd and anxiety/depression under control.\n",
       "106921                                                                                                    that‚Äôs called depression lol https://t.co/fhmut7tupc\n",
       "107568            rt @spacebrat: its funny how u get depressed and just start not talking to people but u missing talking and interacting with then but u jus‚Ä¶\n",
       "107822                                                                rt @itzwikipedia: too much homework can cause stress, depression, and even lower grades.\n",
       "108714            i‚Äôm so tired of being a worthless piece of shit but my depression is literally destroying me and i haven‚Äôt done any‚Ä¶ https://t.co/x9e93ezqmu\n",
       "109421            trump's america wants to get rid of coverage for these drugs because apparently depression is a pre-existing condit‚Ä¶ https://t.co/zzapyrmlvm\n",
       "110060                                                                    rt @rosyfuck: @bbzaria @aljaminbrydie omg did u just end my depression üòãüíóüòçüòäüòçüò§‚òïÔ∏è‚ù§Ô∏è‚ù£Ô∏èü§™\n",
       "110185            this is for those, who are sad, frustrated, alone, depressed because of many reasons like for being unemployed, uns‚Ä¶ https://t.co/scbsnaxi9y\n",
       "111439                                                                                       shoutout to being depressed and malnourished for this weight loss\n",
       "\n",
       "[138 rows x 1 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for tweets that use depression\n",
    "df_2[(df_2['tweets'].str.contains('depressed') | df_2['tweets'].str.contains('depression'))]\n",
    "\n",
    "#drop them\n",
    "df_2.drop(df_2[(df_2.tweets.str.contains('depressed')) | (df_2.tweets.str.contains('depression'))].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111847"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recheck length\n",
    "len(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#export to check quality\n",
    "# df_2_sample = df_2.sample(n=100)\n",
    "# df_2_sample.to_csv('../sample_100_random_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#column of 0's\n",
    "x = 0\n",
    "x = x * len(df_2)\n",
    "\n",
    "df_2['target'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#balance classes\n",
    "df_3 = df.sample(n=len(df_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combine dfs\n",
    "df = pd.concat([df_3,df_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223694"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'rt @ techreview : a neural network can detect depression and mania in bipolar subjects by analyzing how they hold and tap on their smartphone‚Ä¶ '\n"
     ]
    }
   ],
   "source": [
    "#preprocess tweets\n",
    "example_text=\"\"\"'RT @techreview: A neural network can \n",
    "detect depression and mania in bipolar subjects \n",
    "by analyzing how they hold and tap on their smartphone‚Ä¶'\"\"\"\n",
    "\n",
    "# tokenize\n",
    "def tokenize_text(input_text):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "    input_text: a string representing an \n",
    "    individual review\n",
    "        \n",
    "    Returns:\n",
    "    input_token: a list containing stemmed \n",
    "    tokens, with punctutations removed, for \n",
    "    an individual review\n",
    "        \n",
    "    \"\"\"\n",
    "    input_tokens=[]\n",
    "        \n",
    "    # Split sentence\n",
    "    sents=sent_tokenize(input_text)\n",
    "            \n",
    "    # Split word\n",
    "    for sent in sents:\n",
    "        input_tokens+=TreebankWordTokenizer().tokenize(sent)\n",
    "        \n",
    "    return input_tokens\n",
    "\n",
    "\n",
    "# canonicalize\n",
    "def canonicalize_tokens(input_tokens):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    input_tokens: a list containing tokenized \n",
    "    tokens for an individual review\n",
    "    \n",
    "    Returns:\n",
    "    input_tokens: a list containing canonicalized \n",
    "    tokens for an individual review\n",
    "    \n",
    "    \"\"\"\n",
    "    input_tokens=utils.canonicalize_words(input_tokens)\n",
    "    return input_tokens\n",
    "\n",
    "\n",
    "# preprocessor \n",
    "def preprocessor(raw_text):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    raw_text: a string representing an\n",
    "    individual review\n",
    "    \n",
    "    Returns:\n",
    "    preprocessed_text: a string representing \n",
    "    a preprocessed individual review\n",
    "    \n",
    "    \"\"\"\n",
    "    # tokenize\n",
    "    tokens=tokenize_text(raw_text)\n",
    "    \n",
    "    # canonicalize\n",
    "    canonical_tokens=canonicalize_tokens(tokens)\n",
    "    \n",
    "    # rejoin string\n",
    "    preprocessed_text=(\" \").join(canonical_tokens) \n",
    "    return preprocessed_text\n",
    "\n",
    "# example data\n",
    "#input_tokens=tokenize_text(example_text)\n",
    "#print(input_tokens)\n",
    "\n",
    "#canonical_tokens=canonicalize_tokens(input_tokens)\n",
    "#print(canonical_tokens)\n",
    "\n",
    "preprocessed_text=preprocessor(example_text) \n",
    "print(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sklearn stopwords: 318\n",
      "number of nltk stopwords: 179\n",
      "number of total stopwords: 388\n"
     ]
    }
   ],
   "source": [
    "# examine stopwords\n",
    "\n",
    "# sklearn stopwords (frozenset)\n",
    "sklearn_stopwords=stop_words.ENGLISH_STOP_WORDS\n",
    "print(\"number of sklearn stopwords: %d\" %(len(sklearn_stopwords)))\n",
    "#print(sklearn_stopwords)\n",
    "\n",
    "# nltk stopwords (list)\n",
    "nltk_stopwords=stopwords.words(\"english\")\n",
    "print(\"number of nltk stopwords: %d\" %(len(nltk_stopwords)))\n",
    "#print(nltk_stopwords)\n",
    "\n",
    "# combined sklearn, nltk, other stopwords (set)\n",
    "total_stopwords=set(list(sklearn_stopwords.difference(set(nltk_stopwords)))+nltk_stopwords)\n",
    "\n",
    "other_stopwords=[\"DG\", \"DGDG\", \"@\", \"rt\", \"'rt\", \"'\", \":\", \"depression\", \"depressed\", \"RT\"]\n",
    "for w in other_stopwords:\n",
    "    total_stopwords.add(w)\n",
    "    \n",
    "print(\"number of total stopwords: %d\" %(len(total_stopwords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['techreview', 'neural', 'network', 'detect', 'mania', 'bipolar', 'subjects', 'analyzing', 'hold', 'tap', 'smartphone‚Ä¶']\n"
     ]
    }
   ],
   "source": [
    "#look at review w/o stop words\n",
    "new_review = []\n",
    "for i in preprocessed_text.split():\n",
    "    if i in total_stopwords:\n",
    "        continue\n",
    "    else:\n",
    "        new_review.append(i)\n",
    "        \n",
    "print(new_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reset index\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train, test set size: 179006, 44688\n",
      "\n",
      "example:\n",
      "tweet: I know my urge to ghost on everything is the depression but there it is\n",
      "label: 1\n"
     ]
    }
   ],
   "source": [
    "#split into test, train before sampling to belance\n",
    "# using recoded labels\n",
    "#create train, test data\n",
    "df['is_train'] = np.random.uniform(0,1, len(df)) <= .8\n",
    "\n",
    "train_data, test_data = df[df['is_train'] == True], df[df['is_train'] == False]\n",
    "\n",
    "# examine train, test shapes\n",
    "print(\"train, test set size: %d, %d\" %(len(train_data), len(test_data))) # train_data: 129023, test_data: 32256\n",
    "print(\"\")\n",
    "\n",
    "# examine train set examples\n",
    "print(\"example:\")\n",
    "print(\"tweet: %s\" %(train_data.get_value(5,'tweets')))\n",
    "print(\"label: %s\" %(train_data.get_value(5,'target')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    89624\n",
       "0    89382\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check class balance\n",
    "train_data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example:\n",
      "tweet: RT @kittytriplet: @thechew @ABCNetwork I just heard the #chew is being cancelled  I am so upset Please reconsider I am battling depression‚Ä¶\n",
      "label: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"example:\")\n",
    "print(\"tweet: %s\" %(train_data.get_value(32,'tweets')))\n",
    "print(\"label: %s\" %(train_data.get_value(32,'target')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build tf-idf model\n",
    "vec=TfidfVectorizer(preprocessor=preprocessor, ngram_range=(1,3), stop_words=total_stopwords, max_features=10000)\n",
    "vec_train_data=vec.fit_transform(train_data['tweets']) \n",
    "vec_test_data=vec.transform(test_data['tweets']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression f1 score: 0.819\n",
      "logistic regression accuracy score: 0.819\n",
      "logistic regression confusion matrix:\n",
      "[[19295  3112]\n",
      " [ 4950 17293]]\n"
     ]
    }
   ],
   "source": [
    "# train Logistic Regression\n",
    "logit=LogisticRegression(penalty='l2')\n",
    "logit.fit(vec_train_data, train_data['target'])\n",
    "pred_labels=logit.predict(vec_test_data)\n",
    "    \n",
    "# assess model\n",
    "f1=f1_score(test_data['target'], pred_labels, average=\"weighted\") \n",
    "accuracy=accuracy_score(test_data['target'], pred_labels)\n",
    "confusion=confusion_matrix(test_data['target'], pred_labels)\n",
    "print(\"logistic regression f1 score: %.3f\" %(f1))\n",
    "print(\"logistic regression accuracy score: %.3f\" %(accuracy))\n",
    "print(\"logistic regression confusion matrix:\")\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#try Keras\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create integer encoding of docs\n",
    "vocab_size = 100\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in df['tweets']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#try tokenizer instead\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(df['tweets'])\n",
    "vocab_t_size = len(t.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create sequence\n",
    "encoded_t_docs = t.texts_to_sequences(df['tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pad docs to equals size\n",
    "pad = 40\n",
    "# padded_docs = pad_sequences(encoded_docs, maxlen=pad, padding='post')\n",
    "padded_t_docs = pad_sequences(encoded_t_docs, maxlen=pad, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([85,  8, 87,  8, 14, 82, 55, 28, 34, 27, 27, 92, 81, 55, 62, 34, 30,\n",
       "       11, 73, 47, 90, 56, 53, 82,  9,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0], dtype=int32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_docs[11105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# X_train,X_test,Y_train,Y_test = train_test_split(padded_docs, df['target'], test_size=.8)\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(padded_t_docs, df['target'], test_size=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44738, 40)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 40, 32)            11611520  \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 250)               320250    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 11,932,021\n",
      "Trainable params: 11,932,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_size = 32\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(Embedding(vocab_size, embedding_size, input_length=pad))\n",
    "model.add(Embedding(vocab_t_size, embedding_size, input_length=pad))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35790 samples, validate on 8948 samples\n",
      "Epoch 1/3\n",
      "35790/35790 [==============================] - 45s 1ms/step - loss: 0.3004 - acc: 0.8578 - val_loss: 0.1765 - val_acc: 0.9312\n",
      "Epoch 2/3\n",
      " 9344/35790 [======>.......................] - ETA: 30s - loss: 0.0603 - acc: 0.9807"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-6b9f2a15d4cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                     validation_split=0.2)\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Final evaluation of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/benthompson/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/benthompson/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Users/benthompson/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/benthompson/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2332\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/benthompson/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/benthompson/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/benthompson/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/benthompson/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/benthompson/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "epochs=3\n",
    "batch_size=128\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keras_journal = [\"Sometime I feel very alone and anxious\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_journal = t.texts_to_sequences(keras_journal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5874, 8, 85, 163, 332, 9, 2197]]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pad\n",
    "pad = 40\n",
    "padded_journal = pad_sequences(encoded_journal, maxlen=pad, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5874,    8,   85,  163,  332,    9, 2197,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "ynew = model.predict_proba(padded_journal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.44699621]], dtype=float32)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ynew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weights of words that predict depression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>naps</th>\n",
       "      <td>5.900655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>battling</th>\n",
       "      <td>6.385033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cures</th>\n",
       "      <td>6.390737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suicidal</th>\n",
       "      <td>6.418737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nap</th>\n",
       "      <td>6.499706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>6.534502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clinical</th>\n",
       "      <td>6.537225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mental</th>\n",
       "      <td>6.697283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>postpartum</th>\n",
       "      <td>6.738400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suffering</th>\n",
       "      <td>6.996678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suffer</th>\n",
       "      <td>7.021831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seasonal</th>\n",
       "      <td>7.307328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mentalhealth</th>\n",
       "      <td>7.817315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crippling</th>\n",
       "      <td>8.548229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imwtclothing</th>\n",
       "      <td>8.667286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tropical</th>\n",
       "      <td>9.076480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cure</th>\n",
       "      <td>9.826463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suicide</th>\n",
       "      <td>10.179024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cured</th>\n",
       "      <td>11.951388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anxiety</th>\n",
       "      <td>17.240594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Weights of words that predict depression\n",
       "naps                                          5.900655\n",
       "battling                                      6.385033\n",
       "cures                                         6.390737\n",
       "suicidal                                      6.418737\n",
       "nap                                           6.499706\n",
       "sadness                                       6.534502\n",
       "clinical                                      6.537225\n",
       "mental                                        6.697283\n",
       "postpartum                                    6.738400\n",
       "suffering                                     6.996678\n",
       "suffer                                        7.021831\n",
       "seasonal                                      7.307328\n",
       "mentalhealth                                  7.817315\n",
       "crippling                                     8.548229\n",
       "imwtclothing                                  8.667286\n",
       "tropical                                      9.076480\n",
       "cure                                          9.826463\n",
       "suicide                                      10.179024\n",
       "cured                                        11.951388\n",
       "anxiety                                      17.240594"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get top words\n",
    "#look at top 5 weights for each class\n",
    "#get coefficients for all features\n",
    "coef_sq = logit.coef_\n",
    "\n",
    "#get index of top 5 absolute values for each class\n",
    "weight_indx = np.argsort(coef_sq)[:, -20:]\n",
    "\n",
    "#flatten so can use to look up wieghts\n",
    "weight_indx = weight_indx.flatten()\n",
    "\n",
    "#get coefficients based on index\n",
    "weights = coef_sq[:, weight_indx]\n",
    " \n",
    "#get words that match weights based on index\n",
    "vocab = np.array(vec.get_feature_names())[weight_indx]\n",
    "\n",
    "# make table\n",
    "df = pd.DataFrame({'Weights of words that predict depression': weights[0]}\n",
    "                  , index=vocab)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of class 0 and 1:  [[ 0.43736519  0.56263481]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weights of words in sample Journal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>store</th>\n",
       "      <td>-1.171633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sure</th>\n",
       "      <td>-0.457797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>interaction</th>\n",
       "      <td>-0.419084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wonderful</th>\n",
       "      <td>-0.374465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strange</th>\n",
       "      <td>0.326909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weird</th>\n",
       "      <td>0.392027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>today</th>\n",
       "      <td>0.923644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>makes feel</th>\n",
       "      <td>1.083485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>going</th>\n",
       "      <td>1.660564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>makes</th>\n",
       "      <td>1.798566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feel</th>\n",
       "      <td>2.465471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Weights of words in sample Journal\n",
       "store                                 -1.171633\n",
       "sure                                  -0.457797\n",
       "interaction                           -0.419084\n",
       "wonderful                             -0.374465\n",
       "strange                                0.326909\n",
       "weird                                  0.392027\n",
       "today                                  0.923644\n",
       "makes feel                             1.083485\n",
       "going                                  1.660564\n",
       "makes                                  1.798566\n",
       "feel                                   2.465471"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try to make up an example journal\n",
    "journal = \"\"\"Today was wonderful. I had a strange interaction at the store. \n",
    "The cashier seemed irratated. I'm not sure what's going on but it makes me feel weird\"\"\"\n",
    "\n",
    "#score test journal\n",
    "vec_test_example=vec.transform([journal]) \n",
    "print(\"probability of class 0 and 1: \",logit.predict_proba(vec_test_example))\n",
    "\n",
    "#get words and weights from test journal\n",
    "word_idx = np.nonzero(vec_test_example)[1]\n",
    "vocab = np.array(vec.get_feature_names())[word_idx]\n",
    "weights = coef_sq[:, word_idx]\n",
    "df = pd.DataFrame({'Weights of words in sample Journal': weights[0]}\n",
    "                  , index=vocab)\n",
    "df.sort_values(by='Weights of words in sample Journal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_exported_model']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#export tfidf model\n",
    "tfidf_file = 'tfidf_exported_model'\n",
    "joblib.dump(vec, tfidf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logistic_regression_model']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#export logistic regression\n",
    "logistic_regression_file = 'logistic_regression_model'\n",
    "joblib.dump(logit, logistic_regression_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of class 0 and 1:  [[ 0.43736519  0.56263481]]\n"
     ]
    }
   ],
   "source": [
    "#test out exported models against prev sample journal\n",
    "loaded_tfidf = joblib.load('tfidf_exported_model')\n",
    "loaded_lr = joblib.load('logistic_regression_model')\n",
    "\n",
    "#score test journal\n",
    "export_test_example=loaded_tfidf.transform([journal]) \n",
    "print(\"probability of class 0 and 1: \",loaded_lr.predict_proba(export_test_example))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pi",
   "language": "python",
   "name": "pi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
